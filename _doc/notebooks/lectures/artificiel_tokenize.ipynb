{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenisation\n",
        "\n",
        "La tokenisation consiste \u00e0 d\u00e9couper un texte en *token*, le plus souvent des mots. Le notebook utilise un extrait d'un article du monde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
              "<script>\n",
              "function repeat_indent_string(n){\n",
              "    var a = \"\" ;\n",
              "    for ( ; n > 0 ; --n)\n",
              "        a += \"    \";\n",
              "    return a;\n",
              "}\n",
              "// look up into all sections and builds an automated menu //\n",
              "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
              "    var anchors = document.getElementsByClassName(\"section\");\n",
              "    if (anchors.length == 0) {\n",
              "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
              "    }\n",
              "    var i,t;\n",
              "    var text_menu = begin;\n",
              "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
              "    var ind = \"\";\n",
              "    var memo_level = 1;\n",
              "    var href;\n",
              "    var tags = [];\n",
              "    var main_item = 0;\n",
              "    var format_open = 0;\n",
              "    for (i = 0; i <= llast; i++)\n",
              "        tags.push(\"h\" + i);\n",
              "\n",
              "    for (i = 0; i < anchors.length; i++) {\n",
              "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
              "\n",
              "        var child = null;\n",
              "        for(t = 0; t < tags.length; t++) {\n",
              "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
              "            if (r.length > 0) {\n",
              "child = r[0];\n",
              "break;\n",
              "            }\n",
              "        }\n",
              "        if (child == null) {\n",
              "            text_memo += \"null\\n\";\n",
              "            continue;\n",
              "        }\n",
              "        if (anchors[i].hasAttribute(\"id\")) {\n",
              "            // when converted in RST\n",
              "            href = anchors[i].id;\n",
              "            text_memo += \"#1-\" + href;\n",
              "            // passer \u00e0 child suivant (le chercher)\n",
              "        }\n",
              "        else if (child.hasAttribute(\"id\")) {\n",
              "            // in a notebook\n",
              "            href = child.id;\n",
              "            text_memo += \"#2-\" + href;\n",
              "        }\n",
              "        else {\n",
              "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
              "            continue;\n",
              "        }\n",
              "        var title = child.textContent;\n",
              "        var level = parseInt(child.tagName.substring(1,2));\n",
              "\n",
              "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
              "\n",
              "        if ((level < lfirst) || (level > llast)) {\n",
              "            continue ;\n",
              "        }\n",
              "        if (title.endsWith('\u00b6')) {\n",
              "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
              "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
              "        }\n",
              "        if (title.length == 0) {\n",
              "            continue;\n",
              "        }\n",
              "\n",
              "        while (level < memo_level) {\n",
              "            text_menu += end_format + \"</ul>\\n\";\n",
              "            format_open -= 1;\n",
              "            memo_level -= 1;\n",
              "        }\n",
              "        if (level == lfirst) {\n",
              "            main_item += 1;\n",
              "        }\n",
              "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
              "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
              "            continue;\n",
              "        }\n",
              "        while (level > memo_level) {\n",
              "            text_menu += \"<ul>\\n\";\n",
              "            memo_level += 1;\n",
              "        }\n",
              "        text_menu += repeat_indent_string(level-2);\n",
              "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
              "        format_open += 1;\n",
              "    }\n",
              "    while (1 < memo_level) {\n",
              "        text_menu += end_format + \"</ul>\\n\";\n",
              "        memo_level -= 1;\n",
              "        format_open -= 1;\n",
              "    }\n",
              "    text_menu += send;\n",
              "    //text_menu += \"\\n\" + text_memo;\n",
              "\n",
              "    while (format_open > 0) {\n",
              "        text_menu += end_format;\n",
              "        format_open -= 1;\n",
              "    }\n",
              "    return text_menu;\n",
              "};\n",
              "var update_menu = function() {\n",
              "    var sbegin = \"\";\n",
              "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
              "    var send = \"\";\n",
              "    var begin_format = '<li>';\n",
              "    var end_format = '</li>';\n",
              "    var keep_item = -1;\n",
              "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item,\n",
              "       begin_format, end_format);\n",
              "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
              "    menu.innerHTML=text_menu;\n",
              "};\n",
              "window.setTimeout(update_menu,2000);\n",
              "            </script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from jyquickhelper import add_notebook_menu\n",
        "add_notebook_menu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "texte = \"\"\"\n",
        "Mardi 20 f\u00e9vrier, \u00e0 la m\u00e9diath\u00e8que des Mureaux (Yvelines), le chef de l\u2019Etat a accompagn\u00e9 \n",
        "la locataire de la rue de Valois pour la remise officielle du rapport \n",
        "sur les biblioth\u00e8ques, r\u00e9dig\u00e9 par leur ami commun, l\u2019acad\u00e9micien \n",
        "Erik Orsenna, avec le concours de No\u00ebl Corbin, inspecteur g\u00e9n\u00e9ral \n",
        "des affaires culturelles. L\u2019occasion de pr\u00e9senter les premi\u00e8res \n",
        "mesures en faveur d\u2019un \u00ab plan biblioth\u00e8ques \u00bb.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La librairie la plus connue pour faire du traitement du langage naturel est [nltk](https://www.nltk.org/) (ou *Natural Language Toolkit*). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Mardi - 20 - f\u00e9vrier - , - \u00e0 - la - m\u00e9diath\u00e8que - des - Mureaux - ( - Yvelines - ) - , - le - chef - de - l - \u2019 - Etat - a - accompagn\u00e9 - la - locataire - de - la - rue - de - Valois - pour - la - remise - officielle - du - rapport - sur - les - biblioth\u00e8ques - , - r\u00e9dig\u00e9 - par - leur - ami - commun - , - l - \u2019 - acad\u00e9micien - Erik - Orsenna - , - avec - le - concours - de - No\u00ebl - Corbin - , - inspecteur - g\u00e9n\u00e9ral - des - affaires - culturelles - . - L - \u2019 - occasion - de - pr\u00e9senter - les - premi\u00e8res - mesures - en - faveur - d - \u2019 - un - \u00ab - plan - biblioth\u00e8ques - \u00bb - .'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "' - '.join(word_tokenize(texte))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Mardi - 20 - f\u00e9vrier - , - \u00e0 - la - m\u00e9diath\u00e8que - des - Mureaux - ( - Yvelines - ), - le - chef - de - l - \u2019 - Etat - a - accompagn\u00e9 - la - locataire - de - la - rue - de - Valois - pour - la - remise - officielle - du - rapport - sur - les - biblioth\u00e8ques - , - r\u00e9dig\u00e9 - par - leur - ami - commun - , - l - \u2019 - acad\u00e9micien - Erik - Orsenna - , - avec - le - concours - de - No\u00ebl - Corbin - , - inspecteur - g\u00e9n\u00e9ral - des - affaires - culturelles - . - L - \u2019 - occasion - de - pr\u00e9senter - les - premi\u00e8res - mesures - en - faveur - d - \u2019 - un - \u00ab - plan - biblioth\u00e8ques - \u00bb.'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "to = WordPunctTokenizer()\n",
        "' - '.join(to.tokenize(texte))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** word_tokenize\n",
            "\n",
            "--- WordPunctTokenizer\n",
            "\n",
            "***************\n",
            "\n",
            "*** 9,16 ****\n",
            "\n",
            "  Mureaux\n",
            "  (\n",
            "  Yvelines\n",
            "! )\n",
            "! ,\n",
            "  le\n",
            "  chef\n",
            "  de\n",
            "--- 9,15 ----\n",
            "\n",
            "  Mureaux\n",
            "  (\n",
            "  Yvelines\n",
            "! ),\n",
            "  le\n",
            "  chef\n",
            "  de\n",
            "***************\n",
            "\n",
            "*** 77,81 ****\n",
            "\n",
            "  \u00ab\n",
            "  plan\n",
            "  biblioth\u00e8ques\n",
            "! \u00bb\n",
            "! .\n",
            "--- 76,79 ----\n",
            "\n",
            "  \u00ab\n",
            "  plan\n",
            "  biblioth\u00e8ques\n",
            "! \u00bb.\n"
          ]
        }
      ],
      "source": [
        "from difflib import context_diff, ndiff\n",
        "print('\\n'.join(context_diff(word_tokenize(texte),\n",
        "                             to.tokenize(texte), \n",
        "                             fromfile='word_tokenize',\n",
        "                             tofile='WordPunctTokenizer')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La documentation de la librairie [nltk](https://www.nltk.org/) est assez longue et ce n'est pas la plus simple d'acc\u00e8s. [gensim](https://radimrehurek.com/gensim/) est une autre option plus r\u00e9cente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mardi - fevrier - a - la - mediatheque - des - mureaux - yvelines - le - chef - de - l - etat - a - accompagne - la - locataire - de - la - rue - de - valois - pour - la - remise - officielle - du - rapport - sur - les - bibliotheques - redige - par - leur - ami - commun - l - academicien - erik - orsenna - avec - le - concours - de - noel - corbin - inspecteur - general - des - affaires - culturelles - l - occasion - de - presenter - les - premieres - mesures - en - faveur - d - un - plan - bibliotheques'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.utils import tokenize\n",
        "\" - \".join(tokenize(texte, deacc=True, lower=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'chiffres - ch'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.utils import tokenize\n",
        "\" - \".join(tokenize(\"chiffres 20 ch20\", deacc=True, lower=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un dernier module a vu le jour [spacy](https://spacy.io/). On suit l'exemple pr\u00e9sent\u00e9 dans [spacy-101](https://spacy.io/usage/spacy-101). Il faut t\u00e9l\u00e9charger un paquet de ressource depuis [spacy-models](https://spacy.io/usage/models#available). **Note :** sous Windows, il faut faudra ruser et installer le module [fr_core_news_sm](https://spacy.io/models/fr#section-fr_core_news_sm) vous m\u00eame (et bidouiller le fichier setup.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('fr_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc = nlp(texte)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n - Mardi - 20 - f\u00e9vrier - , - \u00e0 - la - m\u00e9diath\u00e8que - des - Mureaux - ( - Yvelines - ) - , - le - chef - de - l\u2019 - Etat - a - accompagn\u00e9 - \\n - la - locataire - de - la - rue - de - Valois - pour - la - remise - officielle - du - rapport - \\n - sur - les - biblioth\u00e8ques - , - r\u00e9dig\u00e9 - par - leur - ami - commun - , - l\u2019 - acad\u00e9micien - \\n - Erik - Orsenna - , - avec - le - concours - de - No\u00ebl - Corbin - , - inspecteur - g\u00e9n\u00e9ral - \\n - des - affaires - culturelles - . - L\u2019 - occasion - de - pr\u00e9senter - les - premi\u00e8res - \\n - mesures - en - faveur - d\u2019 - un - \u00ab - plan - biblioth\u00e8ques - \u00bb - . - \\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' - '.join(t.text for t in doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Mardi - f\u00e9vrier - \u00e0 - la - m\u00e9diath\u00e8que - des - Mureaux - Yvelines - le - chef - de - Etat - a - accompagn\u00e9 - la - locataire - de - la - rue - de - Valois - pour - la - remise - officielle - du - rapport - sur - les - biblioth\u00e8ques - r\u00e9dig\u00e9 - par - leur - ami - commun - acad\u00e9micien - Erik - Orsenna - avec - le - concours - de - No\u00ebl - Corbin - inspecteur - g\u00e9n\u00e9ral - des - affaires - culturelles - occasion - de - pr\u00e9senter - les - premi\u00e8res - mesures - en - faveur - un - plan - biblioth\u00e8ques'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' - '.join(t.text for t in doc if t.is_alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On voit que la tokenisation des apostrophes est diff\u00e9rente et qu'on a plus d'information sur chaque token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "el = doc[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Mardi', True)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "el.text, el.is_alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Supprimer les stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### nltk\n",
        "\n",
        "Le module [nltk](https://www.nltk.org/) fournit une liste de stopwords. Il suffit de supprimer tous les mots dans cette liste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"i - me - my - myself - we - our - ours - ourselves - you - you're - you've - you'll - you'd - your - yours - yourself - yourselves - he - him - his - himself - she - she's - her - hers - herself - it - it's - its - itself - they - them - their - theirs - themselves - what - which - who - whom - this - that - that'll - these - those - am - is - are - was - were - be - been - being - have - has - had - having - do - does - did - doing - a - an - the - and - but - if - or - because - as - until - while - of - at - by - for - with - about - against - between - into - through - during - before - after - above - below - to - from - up - down - in - out - on - off - over - under - again - further - then - once - here - there - when - where - why - how - all - any - both - each - few - more - most - other - some - such - no - nor - not - only - own - same - so - than - too - very - s - t - can - will - just - don - don't - should - should've - now - d - ll - m - o - re - ve - y - ain - aren - aren't - couldn - couldn't - didn - didn't - doesn - doesn't - hadn - hadn't - hasn - hasn't - haven - haven't - isn - isn't - ma - mightn - mightn't - mustn - mustn't - needn - needn't - shan - shan't - shouldn - shouldn't - wasn - wasn't - weren - weren't - won - won't - wouldn - wouldn't\""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "' - '.join(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'au - aux - avec - ce - ces - dans - de - des - du - elle - en - et - eux - il - je - la - le - leur - lui - ma - mais - me - m\u00eame - mes - moi - mon - ne - nos - notre - nous - on - ou - par - pas - pour - qu - que - qui - sa - se - ses - son - sur - ta - te - tes - toi - ton - tu - un - une - vos - votre - vous - c - d - j - l - \u00e0 - m - n - s - t - y - \u00e9t\u00e9 - \u00e9t\u00e9e - \u00e9t\u00e9es - \u00e9t\u00e9s - \u00e9tant - \u00e9tante - \u00e9tants - \u00e9tantes - suis - es - est - sommes - \u00eates - sont - serai - seras - sera - serons - serez - seront - serais - serait - serions - seriez - seraient - \u00e9tais - \u00e9tait - \u00e9tions - \u00e9tiez - \u00e9taient - fus - fut - f\u00fbmes - f\u00fbtes - furent - sois - soit - soyons - soyez - soient - fusse - fusses - f\u00fbt - fussions - fussiez - fussent - ayant - ayante - ayantes - ayants - eu - eue - eues - eus - ai - as - avons - avez - ont - aurai - auras - aura - aurons - aurez - auront - aurais - aurait - aurions - auriez - auraient - avais - avait - avions - aviez - avaient - eut - e\u00fbmes - e\u00fbtes - eurent - aie - aies - ait - ayons - ayez - aient - eusse - eusses - e\u00fbt - eussions - eussiez - eussent'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' - '.join(stopwords.words('french'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Mardi - 20 - f\u00e9vrier - , - m\u00e9diath\u00e8que - Mureaux - ( - Yvelines - ) - , - chef - \u2019 - Etat - a - accompagn\u00e9 - locataire - rue - Valois - remise - officielle - rapport - les - biblioth\u00e8ques - , - r\u00e9dig\u00e9 - ami - commun - , - \u2019 - acad\u00e9micien - Erik - Orsenna - , - concours - No\u00ebl - Corbin - , - inspecteur - g\u00e9n\u00e9ral - affaires - culturelles - . - L - \u2019 - occasion - pr\u00e9senter - les - premi\u00e8res - mesures - faveur - \u2019 - \u00ab - plan - biblioth\u00e8ques - \u00bb - .'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "st = set(stopwords.words('french'))\n",
        "' - '.join(w for w in word_tokenize(texte) if w not in st)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'rather - am - throughout - hereafter - whereby - how - last - full - thereby - as - they - make - the - nowhere - only - please - once - was - since - nobody - sometime - who - else - whom - five - forty - find - will - above - serious - beyond - latterly - fill - should - we - eg - anyway - before - latter - get - then - hasnt - hundred - though - ltd - of - sincere - that - such - bottom - quite - while - not - nevertheless - whence - computer - somewhere - seemed - either - down - amoungst - go - herein - therefore - interest - doing - twelve - first - couldnt - our - regarding - hereupon - whole - seems - nothing - you - no - empty - did - can - about - re - themselves - wherein - another - whatever - thus - someone - in - wherever - but - its - further - more - himself - below - keep - own - he - so - mine - mostly - six - sometimes - between - now - have - these - due - does - could - off - being - becomes - over - i - thence - cry - everywhere - become - some - besides - almost - front - always - done - nine - ie - top - thin - seem - whereas - anywhere - show - hereby - formerly - fire - moreover - call - why - made - therein - much - if - any - thereafter - his - enough - whither - without - still - cant - may - her - give - hers - might - she - fifteen - didn - except - con - upon - do - etc - although - de - yourself - became - next - anything - say - yet - were - bill - me - others - whereafter - four - where - eight - is - very - un - just - becoming - amongst - each - thereupon - km - least - whose - neither - other - here - whereupon - out - yourselves - inc - whoever - part - see - him - put - everyone - twenty - or - thick - yours - mill - itself - beside - anyone - eleven - also - too - because - system - be - namely - don - are - few - again - ours - been - well - most - along - a - three - it - during - whether - co - many - among - indeed - already - with - side - what - under - ten - same - there - towards - name - them - otherwise - take - after - alone - my - via - together - often - everything - every - us - which - using - had - than - ourselves - into - must - doesn - herself - those - an - both - to - behind - somehow - move - back - never - beforehand - and - against - when - detail - however - until - your - all - would - whenever - none - less - at - found - perhaps - noone - has - sixty - toward - across - fifty - cannot - ever - by - afterwards - used - describe - this - meanwhile - anyhow - even - from - something - hence - former - kg - several - through - unless - on - within - onto - third - various - one - elsewhere - for - two - nor - their - really - myself - up - amount - seeming - thru - around - per'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\" - \".join(STOPWORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### spacy\n",
        "\n",
        "Encore plus simple avec [spacy](https://spacy.io/) o\u00f9 chaque token contient l'information souhait\u00e9e."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\u00e0 - la - des - le - de - l\u2019 - a - la - de - la - de - pour - la - du - sur - les - par - leur - l\u2019 - avec - le - de - des - de - les - en - d\u2019 - un'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc = nlp(texte)\n",
        "' - '.join(t.text for t in doc if t.is_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n - Mardi - 20 - f\u00e9vrier - , - m\u00e9diath\u00e8que - Mureaux - ( - Yvelines - ) - , - chef - Etat - accompagn\u00e9 - \\n - locataire - rue - Valois - remise - officielle - rapport - \\n - biblioth\u00e8ques - , - r\u00e9dig\u00e9 - ami - commun - , - acad\u00e9micien - \\n - Erik - Orsenna - , - concours - No\u00ebl - Corbin - , - inspecteur - g\u00e9n\u00e9ral - \\n - affaires - culturelles - . - L\u2019 - occasion - pr\u00e9senter - premi\u00e8res - \\n - mesures - faveur - \u00ab - plan - biblioth\u00e8ques - \u00bb - . - \\n'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "' - '.join(t.text for t in doc if not t.is_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autres modules\n",
        "\n",
        "* [textblob](https://textblob.readthedocs.io/en/dev/), [textblob-fr](https://pypi.python.org/pypi/textblob-fr/0.2.0) : Simplified Text Processing\n",
        "* [corpora](http://pythonhosted.org/Corpora/), [pycorpora](https://github.com/aparrish/pycorpora) : corpus de texte\n",
        "* [regex4dummies](https://pypi.python.org/pypi/regex4dummies/) : expression r\u00e9guli\u00e8re pour extraire des informations dans un texte\n",
        "\n",
        "Il existe une quantit\u00e9 de modules diff\u00e9rentes. Lorsque les sources sont connues et tr\u00e8s utilis\u00e9es comme [wikipedia](https://dumps.wikimedia.org/backup-index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## n-grams\n",
        "\n",
        "Petit interm\u00e8de : apr\u00e8s un d\u00e9coupage en mots, on ne consid\u00e8re plus l'ordre avec une approche *bag-of-words*. Si l'information contenu par l'ordre des mots s'av\u00e8re importante, il faut consid\u00e9rer un d\u00e9coupage en couple de mots (bi-grammes), triplets de mots (3-grammes)..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(None, None, None, 'Mardi'),\n",
              " (None, None, 'Mardi', '20'),\n",
              " (None, 'Mardi', '20', 'f\u00e9vrier'),\n",
              " ('Mardi', '20', 'f\u00e9vrier', ','),\n",
              " ('20', 'f\u00e9vrier', ',', '\u00e0'),\n",
              " ('f\u00e9vrier', ',', '\u00e0', 'la'),\n",
              " (',', '\u00e0', 'la', 'm\u00e9diath\u00e8que')]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.util import ngrams\n",
        "generated_ngrams = ngrams(word_tokenize(texte), 4, pad_left=True, pad_right=True)\n",
        "list(generated_ngrams)[:7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Versions utilis\u00e9es pour ce notebook\n",
        "\n",
        "[spacy](https://spacy.io/) s'est montr\u00e9 quelque peu fantasques cette ann\u00e9e avec quelques erreurs notamment celle-ci :\n",
        "[ValueError: cymem.cymem.Pool has the wrong size, try recompiling](https://github.com/explosion/spaCy/issues/2852). Voici les versions utilis\u00e9es..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def version(module):\n",
        "    try:\n",
        "        ver = getattr(module, '__version__', None)\n",
        "        if ver is None:\n",
        "            ver = [_ for _ in os.listdir(os.path.join(module.__file__, '..', '..')) \\\n",
        "                   if module.__name__ in _][-1]\n",
        "        return ver\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "thinc thinc-6.12.1.dist-info\n",
            "preshed preshed-2.0.1.dist-info\n",
            "cymem cymem-2.0.2.dist-info\n",
            "murmurhash murmurhash-1.0.1.dist-info\n",
            "plac 0.9.6\n",
            "spacy 2.0.18\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import thinc\n",
        "print(\"thinc\", version(thinc))\n",
        "import preshed\n",
        "print(\"preshed\", version(preshed))\n",
        "import cymem\n",
        "print(\"cymem\", version(cymem))\n",
        "import murmurhash\n",
        "print(\"murmurhash\", version(murmurhash))\n",
        "import plac\n",
        "print(\"plac\", plac.__version__)\n",
        "import spacy\n",
        "print(\"spacy\", spacy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}